<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tree Enumeration</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/src/assets/css/theme.css">
    <link rel="stylesheet" href="/src/assets/css/forest-features.css">
    <style>
        .embed-mode .app-header,
        .embed-mode .app-footer { display: none; }
        .embed-mode main.container { padding-top: 0 !important; }
        canvas {
            max-width: 100%;
            height: auto;
            border-radius: 16px;
            box-shadow: 0 8px 24px rgba(0, 0, 0, 0.3);
            border: 2px solid rgba(79, 127, 71, 0.3);
        }
        #uploadInput {
            display: none;
        }
        .forest-upload-wrapper {
            margin: 24px auto;
            max-width: 600px;
        }
    </style>
</head>
<body class="forest-feature-page">
    <script>
        if (window.self !== window.top) {
            document.documentElement.classList.add('embed-mode');
        }
    </script>

    <!-- Forest Background Elements -->
    <div class="forest-silhouette left"></div>
    <div class="forest-silhouette right"></div>
    <div class="forest-particles" id="forestParticles"></div>

    <main class="container" style="padding: 24px 0 40px; position: relative; z-index: 3;">
        
        <!-- Feature Header -->
        <div class="forest-feature-header">
            <div class="forest-header-bar">
                <svg class="forest-header-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M12 2C8 2 5 5 5 9c0 5 7 13 7 13s7-8 7-13c0-4-3-7-7-7z"/>
                </svg>
                <h1 class="forest-header-title">Tree Enumeration</h1>
            </div>
        </div>

        <!-- Upload Section -->
        <div class="forest-content-card">
            <div class="forest-upload-wrapper">
                <label class="forest-upload-area" for="uploadInput">
                    <svg class="forest-upload-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/>
                        <polyline points="17 8 12 3 7 8"/>
                        <line x1="12" y1="3" x2="12" y2="15"/>
                    </svg>
                    <div class="forest-upload-text">Upload Image for Tree Detection</div>
                    <div class="forest-upload-hint">JPG, PNG, WEBP supported</div>
                    <input type="file" id="uploadInput" accept="image/*">
                </label>
            </div>
            
            <div style="text-align: center; margin-top: 24px;">
                <canvas style="display: none;"></canvas>
            </div>
        </div>

        <div id="objectCount" style="position:fixed;left:16px;bottom:16px;background:rgba(15,26,18,0.85);backdrop-filter:blur(10px);color:#7BC66A;padding:12px 20px;border-radius:12px;font-weight:700;font-size:15px;border:2px solid rgba(123,198,106,0.4);box-shadow:0 8px 20px rgba(0,0,0,0.3),0 0 20px rgba(123,198,106,0.2);z-index:9999;">Detected Trees: 0</div>
        <script>
            // ... Your existing JavaScript code ...
       /**
             * "Upload" button onClick handler: uploads selected image file
             * to backend, receives array of detected objects
             * and draws them on top of image
             */
             const input = document.getElementById("uploadInput");
             input.addEventListener("change",async(event) => {
                 const boxes = await detect_objects_on_image(event.target.files[0]);
                 draw_image_and_boxes(event.target.files[0],boxes);
             })
      

            const confidenceThreshold = 0.35; // Minimum confidence shown on the canvas

            /**
             * Function draws the image from provided file
             * and bounding boxes of detected objects on
             * top of the image
             * @param file Uploaded file object
             * @param boxes Array of bounding boxes in format [[x1,y1,x2,y2,object_type,probability],...]
             */
             
  function draw_image_and_boxes(file, boxes) {
  const img = new Image()
  img.src = URL.createObjectURL(file);
  img.onload = () => {
    const canvas = document.querySelector("canvas");
    canvas.width = img.width;
    canvas.height = img.height;
    const ctx = canvas.getContext("2d");
    ctx.drawImage(img, 0, 0);
    ctx.strokeStyle = "#00FF00";
    ctx.lineWidth = 3;
    ctx.font = "18px serif";

    const confidentBoxes = boxes.filter(([, , , , , prob]) => prob >= confidenceThreshold);
    confidentBoxes.forEach(([x1, y1, x2, y2, label, prob]) => {
      ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
      ctx.fillStyle = "#00ff00";
      const text = `${label} (${(prob * 100).toFixed(2)}%)`;
      const width = ctx.measureText(text).width;
      ctx.fillRect(x1, y1, width + 10, 25);
      ctx.fillStyle = "#000000";
      ctx.fillText(text, x1, y1 + 18);
    });

    // Display the count of detected objects
    document.getElementById("objectCount").textContent = `Detected Trees: ${confidentBoxes.length}`;
  }
}



            /**
             * Function receives an image, passes it through YOLOv8 neural network
             * and returns an array of detected objects and their bounding boxes
             * @param buf Input image body
             * @returns Array of bounding boxes in format [[x1,y1,x2,y2,object_type,probability],..]
             */
            async function detect_objects_on_image(buf) {
                const [input,img_width,img_height] = await prepare_input(buf);
                const output = await run_model(input);
                return process_output(output,img_width,img_height);
            }
      
            /**
             * Function used to convert input image to tensor,
             * required as an input to YOLOv8 object detection
             * network.
             * @param buf Content of uploaded file
             * @returns Array of pixels
             */
            async function prepare_input(buf) {
                return new Promise(resolve => {
                    const img = new Image();
                    img.src = URL.createObjectURL(buf);
                    img.onload = () => {
                        const img_width = img.width;
                        const img_height = img.height;

                        // Letterbox resize to 640x640 while preserving aspect ratio
                        const canvas = document.createElement("canvas");
                        canvas.width = 640;
                        canvas.height = 640;
                        const ctx = canvas.getContext("2d");
                        ctx.fillStyle = "#000";
                        ctx.fillRect(0,0,640,640);

                        const scale = Math.min(640 / img_width, 640 / img_height);
                        const newW = Math.round(img_width * scale);
                        const newH = Math.round(img_height * scale);
                        const padX = Math.floor((640 - newW) / 2);
                        const padY = Math.floor((640 - newH) / 2);

                        ctx.drawImage(img, 0, 0, img_width, img_height, padX, padY, newW, newH);

                        const imgData = ctx.getImageData(0,0,640,640);
                        const pixels = imgData.data;
                        const red = [], green = [], blue = [];
                        for (let index=0; index<pixels.length; index+=4) {
                            // Normalize to [0,1]
                            red.push(pixels[index] / 255.0);
                            green.push(pixels[index+1] / 255.0);
                            blue.push(pixels[index+2] / 255.0);
                        }
                        const input = [...red, ...green, ...blue];
                        // Save transform for box mapping in process_output
                        window.__lastScale = scale;
                        window.__lastPadX = padX;
                        window.__lastPadY = padY;
                        // return original dims
                        resolve([ { data: input, scale, padX, padY }, img_width, img_height ])
                    }
                })
            }
      
            /**
             * Function used to pass provided input tensor to YOLOv8 neural network and return result
             * @param input Input pixels array
             * @returns Raw output of neural network as a flat array of numbers
             */
            async function run_model(input) {
                const model = await ort.InferenceSession.create("https://ml-cdn.vipulchaturvedi.com/100going1000.onnx");
                const tensor = new ort.Tensor(Float32Array.from(input.data),[1, 3, 640, 640]);
                const outputs = await model.run({images:tensor});
                return outputs["output0"].data;
            }
      
            /*
             * Function used to convert RAW output from YOLOv8 to an array of detected objects.
             * Each object contain the bounding box of this object, the type of object and the probability
             * @param output Raw output of YOLOv8 network
             * @param img_width Width of original image
             * @param img_height Height of original image
             * @returns Array of detected objects in a format [[x1,y1,x2,y2,object_type,probability],..]
             */
            function process_output(output, img_width, img_height) {
                let boxes = [];
                for (let index=0;index<8400;index++) {
                    const [class_id,prob] = [...Array(80).keys()]
                        .map(col => [col, output[8400*(col+4)+index]])
                        .reduce((accum, item) => item[1]>accum[1] ? item : accum,[0,0]);
                    if (prob < 0.35) {
                        continue;
                    }
                    const label = yolo_classes[class_id];
                    const xc = output[index];
                    const yc = output[8400+index];
                    const w = output[2*8400+index];
                    const h = output[3*8400+index];
                    // Reverse letterbox
                    const scale = window.__lastScale || 1;
                    const padX = window.__lastPadX || 0;
                    const padY = window.__lastPadY || 0;
                    const x1n = (xc - w/2) - padX;
                    const y1n = (yc - h/2) - padY;
                    const x2n = (xc + w/2) - padX;
                    const y2n = (yc + h/2) - padY;
                    const x1 = (x1n/ (640 - 0)) * (img_width / scale);
                    const y1 = (y1n/ (640 - 0)) * (img_height / scale);
                    const x2 = (x2n/ (640 - 0)) * (img_width / scale);
                    const y2 = (y2n/ (640 - 0)) * (img_height / scale);
                    boxes.push([x1, y1, x2, y2, label, prob]);
                }
      
                boxes = boxes.sort((box1,box2) => box2[5]-box1[5])
                const result = [];
                while (boxes.length>0) {
                    result.push(boxes[0]);
                    boxes = boxes.filter(box => iou(boxes[0],box)<0.6);
                }
                return result;
            }
      
            /**
             * Function calculates "Intersection-over-union" coefficient for specified two boxes
             * https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/.
             * @param box1 First box in format: [x1,y1,x2,y2,object_class,probability]
             * @param box2 Second box in format: [x1,y1,x2,y2,object_class,probability]
             * @returns Intersection over union ratio as a float number
             */
            function iou(box1,box2) {
                return intersection(box1,box2)/union(box1,box2);
            }
      
            /**
             * Function calculates union area of two boxes.
             *     :param box1: First box in format [x1,y1,x2,y2,object_class,probability]
             *     :param box2: Second box in format [x1,y1,x2,y2,object_class,probability]
             *     :return: Area of the boxes union as a float number
             * @param box1 First box in format [x1,y1,x2,y2,object_class,probability]
             * @param box2 Second box in format [x1,y1,x2,y2,object_class,probability]
             * @returns Area of the boxes union as a float number
             */
            function union(box1,box2) {
                const [box1_x1,box1_y1,box1_x2,box1_y2] = box1;
                const [box2_x1,box2_y1,box2_x2,box2_y2] = box2;
                const box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)
                const box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)
                return box1_area + box2_area - intersection(box1,box2)
            }
      
            /**
             * Function calculates intersection area of two boxes
             * @param box1 First box in format [x1,y1,x2,y2,object_class,probability]
             * @param box2 Second box in format [x1,y1,x2,y2,object_class,probability]
             * @returns Area of intersection of the boxes as a float number
             */
            function intersection(box1,box2) {
                const [box1_x1,box1_y1,box1_x2,box1_y2] = box1;
                const [box2_x1,box2_y1,box2_x2,box2_y2] = box2;
                const x1 = Math.max(box1_x1,box2_x1);
                const y1 = Math.max(box1_y1,box2_y1);
                const x2 = Math.min(box1_x2,box2_x2);
                const y2 = Math.min(box1_y2,box2_y2);
                return (x2-x1)*(y2-y1)
            }
      
            /**
             * Array of YOLOv8 class labels
             */
            const yolo_classes = [
                'tree'
            ];
            /**
             * Function draws the image from provided file
             * and bounding boxes of detected objects on
             * top of the image
             * @param file Uploaded file object
             * @param boxes Array of bounding boxes in format [[x1,y1,x2,y2,object_type,probability],...]
             */
            function draw_image_and_boxes(file, boxes) {
                const img = new Image()
                img.src = URL.createObjectURL(file);
                img.onload = () => {
                    const canvas = document.querySelector("canvas");
                    canvas.width = img.width;
                    canvas.height = img.height;
                    const ctx = canvas.getContext("2d");
                    ctx.drawImage(img,0,0);
                    const confidentBoxes = boxes.filter(([, , , , , prob]) => prob >= confidenceThreshold);
                    ctx.strokeStyle = "#00FF00";
                    ctx.lineWidth = 3;
                    ctx.font = "18px serif";
                    confidentBoxes.forEach(([x1,y1,x2,y2,label,prob]) => {
                        ctx.strokeRect(x1,y1,x2-x1,y2-y1);
                        ctx.fillStyle = "#00ff00";
                        const text = `${label} (${(prob * 100).toFixed(2)}%)`;
                        const width = ctx.measureText(text).width;
                        ctx.fillRect(x1,y1,width+10,25);
                        ctx.fillStyle = "#000000";
                        ctx.fillText(text, x1, y1+18);
                    });
      
                    // Display the count of detected objects
                    document.getElementById("objectCount").textContent = `Detected Trees: ${confidentBoxes.length}`;
                }
            }
      
            // ... Rest of your code ...
      
          </script>
        <script>
            // Initialize forest particles
            function initForestParticles() {
                const particlesContainer = document.getElementById('forestParticles');
                if (!particlesContainer) return;
                
                const particleCount = 18;
                for (let i = 0; i < particleCount; i++) {
                    const particle = document.createElement('div');
                    particle.className = 'forest-particle';
                    particle.style.left = Math.random() * 100 + '%';
                    particle.style.top = Math.random() * 100 + '%';
                    particle.style.animationDelay = Math.random() * 35 + 's';
                    particle.style.animationDuration = (30 + Math.random() * 20) + 's';
                    const size = 5 + Math.random() * 4;
                    particle.style.width = size + 'px';
                    particle.style.height = size + 'px';
                    particlesContainer.appendChild(particle);
                }
            }

            // Update canvas display
            const originalDraw = draw_image_and_boxes;
            draw_image_and_boxes = function(file, boxes) {
                originalDraw(file, boxes);
                const canvas = document.querySelector("canvas");
                if (canvas) {
                    canvas.style.display = "block";
                    canvas.style.margin = "24px auto";
                }
            };

            document.addEventListener('DOMContentLoaded', () => {
                initForestParticles();
            });
        </script>
    </main>
</body>
</html>